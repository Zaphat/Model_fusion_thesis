{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9979035,"sourceType":"datasetVersion","datasetId":6140254}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T18:43:25.782866Z","iopub.execute_input":"2024-11-23T18:43:25.783983Z","iopub.status.idle":"2024-11-23T18:43:37.378485Z","shell.execute_reply.started":"2024-11-23T18:43:25.783927Z","shell.execute_reply":"2024-11-23T18:43:37.377232Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\n\nmodel_name=\"facebook/wav2vec2-base\"\nmodel = Wav2Vec2Model.from_pretrained(model_name)\nprocessor = Wav2Vec2Processor.from_pretrained(model_name)\ndef extract_audio_features(audio_path, processor, model):\n    \"\"\"SIZE [N,768]\n    N (Sequence length dimension):input audio length \n    \"\"\"\n    # Load the audio file\n    waveform, sample_rate = torchaudio.load(audio_path)\n    \n    # Convert to mono if stereo\n    if waveform.shape[0] > 1:\n        waveform = torch.mean(waveform, dim=0, keepdim=True)\n    \n    # Resample if necessary\n    if sample_rate != processor.feature_extractor.sampling_rate:\n        resampler = torchaudio.transforms.Resample(\n            sample_rate, \n            processor.feature_extractor.sampling_rate\n        )\n        waveform = resampler(waveform)\n\n    # Define a silence threshold (e.g., 0.01 for low amplitude sounds)\n    threshold = 0.01\n    \n    # Detect silent frames by checking if the absolute amplitude is below the threshold\n    silent_frames = torch.abs(waveform) < threshold\n    \n    # Calculate the percentage of silent frames\n    silent_percentage = silent_frames.float().mean().item() * 100\n    \n    # Set a percentage threshold (e.g., if 95% of the audio is silent, consider it silent)\n    silence_threshold = 90\n    if silent_percentage > silence_threshold:\n        return torch.zeros((0, model.config.hidden_size))\n        \n    # Prepare input\n    input_values = processor(\n        waveform.squeeze().numpy(), \n        sampling_rate=processor.feature_extractor.sampling_rate, \n        return_tensors=\"pt\"\n    ).input_values\n    \n    # Extract features\n    with torch.no_grad():\n        outputs = model(input_values)\n        # Get the last hidden states\n        features = outputs.last_hidden_state\n        \n    return features.squeeze(0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_audio_features(\"/kaggle/input/mmhate/HateMM/audio/non_hate_video_73.wav\", processor, model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport h5py\nimport numpy as np\nfrom transformers import ViTModel, ViTImageProcessor\n\n# Initialize model and processor\nmodel_name = \"google/vit-base-patch16-224\"\nmodel = ViTModel.from_pretrained(model_name)\nprocessor = ViTImageProcessor.from_pretrained(model_name)\n\ndef extract_image_features(h5_path, processor, model, batch_size=32):\n    \"\"\"SIZE [N,768]\n    N is the number of images\"\"\"\n    # Load images from h5py file\n    with h5py.File(h5_path, 'r') as f:\n        frames = f['frames'][:]  # Load all frames\n    # Convert frames to RGB if necessary\n    if len(frames.shape) == 3:  # If single channel\n        frames = np.stack([frames] * 3, axis=-1)\n    \n    # Process frames in batches\n    all_features = []\n    for i in range(0, len(frames), batch_size):\n        batch_frames = frames[i:i + batch_size]\n        \n        # Prepare input\n        inputs = processor(\n            images=batch_frames,\n            return_tensors=\"pt\",\n            padding=True\n        )\n        \n        # Extract features\n        with torch.no_grad():\n            outputs = model(inputs.pixel_values)\n            # Get the pooled features\n            features = outputs.pooler_output\n            all_features.append(features)\n\n    # Concatenate all features\n    features = torch.cat(all_features, dim=0)\n    return features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_image_features(\"/kaggle/input/mmhate/HateMM/image_sequences/hate_video_122.h5\",processor, model).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel = SentenceTransformer(model_name)\n\ndef extract_text_features(txt_path, model):\n    \"\"\"size [N, 768] where N is the number of sentences in the transcript.\n    \"\"\"\n    \n    # Load text from file\n    with open(txt_path, 'r', encoding='utf-8') as f:\n        text = f.read()\n    \n    # Split text into sentences\n    sentences = [s.strip() for s in text.split('. ') if s.strip()] \n    \n    # empty transcript (silent/only-music audio)\n    if not sentences:\n        return torch.zeros((0, model.config.hidden_size))\n    \n    embeddings = model.encode(sentences)\n    return embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_text_features(\"/kaggle/input/mmhate/HateMM/transcripts/hate_video_95.txt\", model).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import (\n    Wav2Vec2Processor, \n    Wav2Vec2Model,\n    ViTModel, \n    ViTImageProcessor\n)\nfrom sentence_transformers import SentenceTransformer\nimport torchaudio\nimport h5py\nimport numpy as np\n\nclass MultimodalHateClassifier(nn.Module):\n    def __init__(\n        self,\n        hidden_size=768,  # All extractors output 768-dim features\n        fusion_hidden_size=512,\n        num_heads=8,\n        dropout=0.1\n    ):\n        super().__init__()\n        \n        # Initialize feature extractors\n        self._init_extractors()\n        \n        # Modality-specific encoders to process variable-length sequences\n        self.audio_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_size,\n                nhead=num_heads,\n                dim_feedforward=fusion_hidden_size,\n                dropout=dropout,\n                batch_first=True\n            ),\n            num_layers=2\n        )\n        \n        self.vision_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_size,\n                nhead=num_heads,\n                dim_feedforward=fusion_hidden_size,\n                dropout=dropout,\n                batch_first=True\n            ),\n            num_layers=2\n        )\n        \n        self.text_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_size,\n                nhead=num_heads,\n                dim_feedforward=fusion_hidden_size,\n                dropout=dropout,\n                batch_first=True\n            ),\n            num_layers=2\n        )\n        \n        # Cross-modal attention fusion\n        self.cross_modal_attention = nn.MultiheadAttention(\n            embed_dim=hidden_size,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        # Final classification layers\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(hidden_size * 3, fusion_hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(fusion_hidden_size, fusion_hidden_size // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(fusion_hidden_size // 2, 1)\n        )\n\n    def _init_extractors(self):\n        \"\"\"Initialize all feature extractors as private members\"\"\"\n        # Audio extractor\n        self._wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n        self._wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n        \n        # Vision extractor\n        self._vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n        self._vit_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n        \n        # Text extractor\n        self._text_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n        \n        # Freeze extractors\n        for param in self._wav2vec_model.parameters():\n            param.requires_grad = False\n        for param in self._vit_model.parameters():\n            param.requires_grad = False\n        for param in self._text_model.parameters():\n            param.requires_grad = False\n\n    def _extract_audio_features(self, audio_path):\n        \"\"\"Extract audio features using Wav2Vec2\"\"\"\n        # Load and preprocess audio\n        waveform, sample_rate = torchaudio.load(audio_path)\n        \n        # Convert to mono if stereo\n        if waveform.shape[0] > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)\n        \n        # Resample if necessary\n        if sample_rate != self._wav2vec_processor.feature_extractor.sampling_rate:\n            resampler = torchaudio.transforms.Resample(\n                sample_rate, \n                self._wav2vec_processor.feature_extractor.sampling_rate\n            )\n            waveform = resampler(waveform)\n\n        # Check for silence\n        threshold = 0.01\n        silent_frames = torch.abs(waveform) < threshold\n        silent_percentage = silent_frames.float().mean().item() * 100\n        \n        if silent_percentage > 90:\n            return torch.zeros((0, self._wav2vec_model.config.hidden_size))\n            \n        # Process audio\n        input_values = self._wav2vec_processor(\n            waveform.squeeze().numpy(), \n            sampling_rate=self._wav2vec_processor.feature_extractor.sampling_rate, \n            return_tensors=\"pt\"\n        ).input_values\n        \n        # Extract features\n        with torch.no_grad():\n            outputs = self._wav2vec_model(input_values)\n            features = outputs.last_hidden_state\n            \n        return features.squeeze(0)\n\n    def _extract_image_features(self, h5_path, batch_size=32):\n        \"\"\"Extract image features using ViT\"\"\"\n        # Load images from h5py file\n        with h5py.File(h5_path, 'r') as f:\n            frames = f['frames'][:]\n            \n        # Convert frames to RGB if necessary\n        if len(frames.shape) == 3:\n            frames = np.stack([frames] * 3, axis=-1)\n        \n        # Process frames in batches\n        all_features = []\n        for i in range(0, len(frames), batch_size):\n            batch_frames = frames[i:i + batch_size]\n            \n            # Prepare input\n            inputs = self._vit_processor(\n                images=batch_frames,\n                return_tensors=\"pt\",\n                padding=True\n            )\n            \n            # Extract features\n            with torch.no_grad():\n                outputs = self._vit_model(inputs.pixel_values)\n                features = outputs.last_hidden_state[:, 0, :]  # Use CLS token\n                all_features.append(features)\n        \n        # Concatenate all features\n        features = torch.cat(all_features, dim=0)\n        return features\n\n    def _extract_text_features(self, txt_path):\n        \"\"\"Extract text features using SentenceTransformer\"\"\"\n        # Load and preprocess text\n        with open(txt_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n        \n        # Split into sentences\n        sentences = [s.strip() for s in text.split('. ') if s.strip()]\n        \n        if not sentences:\n            return torch.zeros((0, self._text_model.config.hidden_size)) \n        \n        # Extract features\n        with torch.no_grad():\n            embeddings = self._text_model.encode(sentences)\n            features = torch.tensor(embeddings)\n            \n        return features\n\n    def _apply_attention_pooling(self, features, encoder):\n        \"\"\"Apply transformer encoding and attention pooling to get sequence representation\"\"\"\n        if features.size(0) == 0:  # Handle empty sequences\n            return torch.zeros(1, features.size(-1))\n            \n        # Add positional encoding\n        pos = torch.arange(0, features.size(1)).unsqueeze(0)\n        pos_encoding = torch.zeros_like(features[0])\n        pos_encoding[:, 0::2] = torch.sin(pos.float() / 10000 ** (torch.arange(0, features.size(-1), 2).float() / features.size(-1)))\n        pos_encoding[:, 1::2] = torch.cos(pos.float() / 10000 ** (torch.arange(1, features.size(-1), 2).float() / features.size(-1)))\n        \n        features = features + pos_encoding\n        \n        # Apply transformer encoding\n        features = encoder(features.unsqueeze(0))\n        \n        # Apply attention pooling\n        attention_weights = torch.softmax(\n            torch.matmul(features, features.transpose(-2, -1)) / np.sqrt(features.size(-1)),\n            dim=-1\n        )\n        pooled = torch.matmul(attention_weights, features).mean(dim=1)\n        \n        return pooled\n\n    def forward(self, audio_path, image_path, text_path):\n        \"\"\"Forward pass through the multimodal architecture\"\"\"\n        # Extract features from each modality\n        audio_features = self._extract_audio_features(audio_path)\n        image_features = self._extract_image_features(image_path)\n        text_features = self._extract_text_features(text_path)\n        \n        # Process each modality sequence\n        audio_encoded = self._apply_attention_pooling(audio_features, self.audio_encoder)\n        vision_encoded = self._apply_attention_pooling(image_features, self.vision_encoder)\n        text_encoded = self._apply_attention_pooling(text_features, self.text_encoder)\n        \n        # Concatenate encoded features\n        combined_features = torch.cat([audio_encoded, vision_encoded, text_encoded], dim=-1)\n        \n        # Final classification\n        logits = self.fusion_layer(combined_features)\n        return torch.sigmoid(logits)\n\n    def predict(self, audio_path, image_path, text_path):\n        \"\"\"Convenience method for making predictions\"\"\"\n        self.eval()\n        with torch.no_grad():\n            output = self.forward(audio_path, image_path, text_path)\n            prediction = (output >= 0.5).int().item()\n            confidence = output.item()\n        return {\n            'prediction': 'hate' if prediction == 1 else 'non-hate',\n            'confidence': confidence\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}