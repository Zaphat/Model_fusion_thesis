{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5549c29d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:00:16.396154Z",
     "iopub.status.busy": "2024-10-25T06:00:16.395384Z",
     "iopub.status.idle": "2024-10-25T06:00:30.976629Z",
     "shell.execute_reply": "2024-10-25T06:00:30.974681Z"
    },
    "papermill": {
     "duration": 14.593885,
     "end_time": "2024-10-25T06:00:30.979902",
     "exception": false,
     "start_time": "2024-10-25T06:00:16.386017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6543b7d",
   "metadata": {
    "papermill": {
     "duration": 0.006652,
     "end_time": "2024-10-25T06:00:30.993870",
     "exception": false,
     "start_time": "2024-10-25T06:00:30.987218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## IMPORT LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a35011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:00:31.008712Z",
     "iopub.status.busy": "2024-10-25T06:00:31.008231Z",
     "iopub.status.idle": "2024-10-25T06:00:41.547289Z",
     "shell.execute_reply": "2024-10-25T06:00:41.546102Z"
    },
    "papermill": {
     "duration": 10.549767,
     "end_time": "2024-10-25T06:00:41.550045",
     "exception": false,
     "start_time": "2024-10-25T06:00:31.000278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.amp import autocast\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import h5py\n",
    "from einops import rearrange\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2177f",
   "metadata": {
    "papermill": {
     "duration": 0.00592,
     "end_time": "2024-10-25T06:00:41.562175",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.556255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## REPRODUCTIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0023b440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:00:41.578028Z",
     "iopub.status.busy": "2024-10-25T06:00:41.577343Z",
     "iopub.status.idle": "2024-10-25T06:00:41.593310Z",
     "shell.execute_reply": "2024-10-25T06:00:41.591982Z"
    },
    "papermill": {
     "duration": 0.026359,
     "end_time": "2024-10-25T06:00:41.595943",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.569584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set environment variable\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "global_seed = 0\n",
    "\n",
    "random.seed(global_seed)\n",
    "np.random.seed(global_seed)\n",
    "\n",
    "torch.manual_seed(global_seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768acb5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:00:41.615749Z",
     "iopub.status.busy": "2024-10-25T06:00:41.615008Z",
     "iopub.status.idle": "2024-10-25T06:00:41.631406Z",
     "shell.execute_reply": "2024-10-25T06:00:41.630557Z"
    },
    "papermill": {
     "duration": 0.031207,
     "end_time": "2024-10-25T06:00:41.634447",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.603240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_config(years: List[int], state: str, state_ansi: str, fips: str, crop_type: str, grow_season: List[int]):\n",
    "    \"\"\"\n",
    "    Creates configuration for winter wheat data collection\n",
    "    grow_season: List containing [start_month, end_month] of the growing cycle\n",
    "                 For winter wheat, this spans across year boundary\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"FIPS\": fips,\n",
    "        \"years\": years,\n",
    "        \"state\": state.upper(),\n",
    "        \"crop_type\": crop_type,\n",
    "        \"data\": {\n",
    "            \"HRRR\": {\n",
    "                \"short_term\": []\n",
    "            },\n",
    "            \"USDA\": [],\n",
    "            \"sentinel\": []\n",
    "        }\n",
    "    }\n",
    "   \n",
    "    for year in years:\n",
    "        # HRRR data - need to consider months from previous year's fall\n",
    "        hrrr_files = []\n",
    "        # Previous year's fall months (planting)\n",
    "        for month in range(9, 13):  # September to December\n",
    "            hrrr_files.append(f\"HRRR/{year-1}/{state.upper()}/HRRR_{state_ansi}_{state.upper()}_{year-1}-{month:02d}.csv\")\n",
    "        # Current year's winter and spring months (growing and harvest)\n",
    "        for month in range(1, 7):  # January to July\n",
    "            hrrr_files.append(f\"HRRR/{year}/{state.upper()}/HRRR_{state_ansi}_{state.upper()}_{year}-{month:02d}.csv\")\n",
    "        \n",
    "        config[\"data\"][\"HRRR\"][\"short_term\"].append(hrrr_files)\n",
    "       \n",
    "        # USDA data\n",
    "        config[\"data\"][\"USDA\"].append(f\"USDA/{crop_type}/{year}/USDA_WinterWheat_County_{year}.csv\")\n",
    "       \n",
    "        # Sentinel data - need to cover previous fall to current summer\n",
    "        quarters = [\n",
    "            # Previous year quarters\n",
    "            (f\"{year-1}-10-01\", f\"{year-1}-12-31\"),  # Q4 (planting)\n",
    "            # Current year quarters\n",
    "            (f\"{year}-01-01\", f\"{year}-03-31\"),      # Q1 (winter growth)\n",
    "            (f\"{year}-04-01\", f\"{year}-06-30\"),      # Q2 (spring growth)\n",
    "        ]\n",
    "       \n",
    "        sentinel_files = []\n",
    "        for start, end in quarters:\n",
    "            sentinel_files.append(f\"AG/{state.upper()}/{start[:4]}/Agriculture_{state_ansi}_{state.upper()}_{start}_{end}.h5\")\n",
    "       \n",
    "        config[\"data\"][\"sentinel\"].append(sentinel_files)\n",
    "   \n",
    "    return config\n",
    "\n",
    "# Train\n",
    "years = list(range(2018, 2022))\n",
    "state = \"IL\"\n",
    "state_ansi = \"17\"\n",
    "fips = ['17011', '17013', '17023', '17025', '17027', '17033', '17037', '17047', '17049', '17067', \n",
    "        '17083', '17089', '17095', '17119', '17121', '17125', '17133', '17141', '17157', '17159', \n",
    "        '17163', '17173', '17177', '17179', '17189', '17201']\n",
    "crop_type = \"WinterWheat\"\n",
    "grow_season = [9, 6]  # September to July (spanning across years)\n",
    "\n",
    "train_config = make_config(years, state, state_ansi, fips, crop_type, grow_season)\n",
    "with open('train_config.json', 'w') as file:\n",
    "    json.dump(train_config, file)\n",
    "\n",
    "# Test\n",
    "years = [2022]\n",
    "test_config = make_config(years, state, state_ansi, fips, crop_type, grow_season)\n",
    "with open('test_config.json', 'w') as file:\n",
    "    json.dump(test_config, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d5219",
   "metadata": {
    "papermill": {
     "duration": 0.008089,
     "end_time": "2024-10-25T06:00:41.651328",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.643239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb334b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:00:41.671246Z",
     "iopub.status.busy": "2024-10-25T06:00:41.670636Z",
     "iopub.status.idle": "2024-10-25T06:00:41.706338Z",
     "shell.execute_reply": "2024-10-25T06:00:41.704876Z"
    },
    "papermill": {
     "duration": 0.048334,
     "end_time": "2024-10-25T06:00:41.710171",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.661837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sentinel2Imagery(Dataset):\n",
    "    def __init__(self, base_dir, config_file, transform=None):\n",
    "        self.transform = transform\n",
    "        self.base_dir = base_dir\n",
    "        \n",
    "        with open(config_file, 'r') as f:\n",
    "            obj = json.load(f)\n",
    "        \n",
    "        self.fips_codes = obj[\"FIPS\"]\n",
    "        self.years = obj[\"years\"]\n",
    "        self.file_paths = obj[\"data\"][\"sentinel\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fips_codes) * len(self.years)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fips_index = index // len(self.years)\n",
    "        year_index = index % len(self.years)\n",
    "        \n",
    "        fips_code = self.fips_codes[fips_index]\n",
    "        year = self.years[year_index]\n",
    "        file_paths = self.file_paths[year_index]\n",
    "        \n",
    "        temporal_list = []\n",
    "        for file_path in file_paths:\n",
    "            with h5py.File(os.path.join(self.base_dir, file_path), 'r') as hf:\n",
    "                groups = hf[fips_code]\n",
    "                for d in groups.keys():\n",
    "                    grids = groups[d][\"data\"]\n",
    "                    grids = torch.from_numpy(np.asarray(grids))\n",
    "                    temporal_list.append(grids)\n",
    "                hf.close()\n",
    "        x = torch.stack(temporal_list)\n",
    "        x = x.to(torch.float32)\n",
    "        x = rearrange(x, 't g h w c -> t g c h w')\n",
    "        if self.transform:\n",
    "            t, g, _, _, _ = x.shape\n",
    "            x = rearrange(x, 't g c h w -> (t g) c h w')\n",
    "            x = self.transform(x)\n",
    "            x = rearrange(x, '(t g) c h w -> t g c h w', t=t, g=g)\n",
    "        return x, fips_code, year\n",
    "\n",
    "class HRRRComputedDataset(Dataset):\n",
    "    def __init__(self, base_dir, config_file, column_names=None):\n",
    "        self.base_dir = base_dir\n",
    "        self.day_range = [i + 1 for i in range(28)]\n",
    "        \n",
    "        with open(config_file, 'r') as f:\n",
    "            obj = json.load(f)\n",
    "        \n",
    "        self.fips_codes = obj[\"FIPS\"]\n",
    "        self.years = obj[\"years\"]\n",
    "        self.short_term_file_path = obj[\"data\"][\"HRRR\"][\"short_term\"]\n",
    "        \n",
    "        if column_names:\n",
    "            self.column_names = column_names\n",
    "        else:\n",
    "            self.column_names = [\n",
    "                'Avg Temperature (K)', 'Max Temperature (K)', 'Min Temperature (K)',\n",
    "                'Precipitation (kg m**-2)', 'Relative Humidity (%)', 'Wind Gust (m s**-1)',\n",
    "                'Wind Speed (m s**-1)', 'Downward Shortwave Radiation Flux (W m**-2)',\n",
    "                'Vapor Pressure Deficit (kPa)'\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fips_codes) * len(self.years)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fips_index = index // len(self.years)\n",
    "        year_index = index % len(self.years)\n",
    "        \n",
    "        fips_code = self.fips_codes[fips_index]\n",
    "        year = self.years[year_index]\n",
    "        short_term_file_paths = self.short_term_file_path[year_index]\n",
    "        x_short = self.get_short_term_val(fips_code, short_term_file_paths)\n",
    "        x_short = x_short.to(torch.float32)\n",
    "        return x_short, fips_code, year\n",
    "\n",
    "    def get_short_term_val(self, fips_code, file_paths):\n",
    "        df_list = []\n",
    "        for file_path in file_paths:\n",
    "            tmp_df = pd.read_csv(os.path.join(self.base_dir, file_path))\n",
    "            df_list.append(tmp_df)\n",
    "\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        df[\"FIPS Code\"] = df[\"FIPS Code\"].astype(str).str.zfill(5)\n",
    "        df = df[(df[\"FIPS Code\"] == fips_code) & (df[\"Daily/Monthly\"] == \"Daily\")]\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        group_month = df.groupby(['Month'])\n",
    "\n",
    "        temporal_list = []\n",
    "        for month, df_month in group_month:\n",
    "            group_grid = df_month.groupby(['Grid Index'])\n",
    "\n",
    "            time_series = []\n",
    "            for grid, df_grid in group_grid:\n",
    "                df_grid = df_grid.sort_values(by=['Day'], ascending=[True], na_position='first')\n",
    "                df_grid = df_grid[df_grid.Day.isin(self.day_range)]\n",
    "                df_grid = df_grid[self.column_names]\n",
    "                val = self.signed_log_transform(torch.from_numpy(df_grid.values))\n",
    "                time_series.append(val)\n",
    "\n",
    "            temporal_list.append(torch.stack(time_series))\n",
    "\n",
    "        x_short = torch.stack(temporal_list)\n",
    "        x_short = rearrange(x_short, 'm g d p -> m d g p')\n",
    "        return x_short\n",
    "\n",
    "    def signed_log_transform(self, data):\n",
    "        epsilon = 1e-9  # small constant to avoid log(0)\n",
    "        return torch.sign(data) * torch.log10(torch.abs(data) + epsilon)\n",
    "\n",
    "class USDACropDataset(Dataset):\n",
    "    def __init__(self, base_dir, config_file, crop_type):\n",
    "        self.base_dir = base_dir\n",
    "        self.crop_type = crop_type\n",
    "        \n",
    "        with open(config_file, 'r') as f:\n",
    "            obj = json.load(f)\n",
    "        \n",
    "        self.fips_codes = obj[\"FIPS\"]\n",
    "        self.years = obj[\"years\"]\n",
    "        self.file_paths = obj[\"data\"][\"USDA\"]\n",
    "\n",
    "        if crop_type == \"Cotton\":\n",
    "            self.column_names = ['PRODUCTION, MEASURED IN 480 LB BALES', 'YIELD, MEASURED IN LB / ACRE']\n",
    "        else:\n",
    "            self.column_names = ['PRODUCTION, MEASURED IN BU', 'YIELD, MEASURED IN BU / ACRE']\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fips_codes) * len(self.years)\n",
    "    def get_num_classes(self):\n",
    "        return len(self.fips_encoder.classes_)\n",
    "    def __getitem__(self, index):\n",
    "        fips_index = index // len(self.years)\n",
    "        year_index = index % len(self.years)\n",
    "        \n",
    "        fips_code = self.fips_codes[fips_index]\n",
    "        year = self.years[year_index]\n",
    "        file_path = self.file_paths[year_index]\n",
    "        df = pd.read_csv(os.path.join(self.base_dir, file_path))\n",
    "\n",
    "        df['state_ansi'] = df['state_ansi'].astype(str).str.zfill(2)\n",
    "        df['county_ansi'] = df['county_ansi'].astype(str).str.zfill(3)\n",
    "\n",
    "        df = df[(df[\"state_ansi\"] == fips_code[:2]) & (df[\"county_ansi\"] == fips_code[-3:])]\n",
    "\n",
    "        df = df[self.column_names]\n",
    "        x = torch.from_numpy(df.values)\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.log(torch.flatten(x, start_dim=0))\n",
    "        return x, fips_code, year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c316669",
   "metadata": {
    "papermill": {
     "duration": 0.006465,
     "end_time": "2024-10-25T06:00:41.722829",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.716364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODAL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b3339d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:00:41.739424Z",
     "iopub.status.busy": "2024-10-25T06:00:41.738981Z",
     "iopub.status.idle": "2024-10-25T06:00:41.761638Z",
     "shell.execute_reply": "2024-10-25T06:00:41.760399Z"
    },
    "papermill": {
     "duration": 0.034475,
     "end_time": "2024-10-25T06:00:41.764011",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.729536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomForestCropPredictor:\n",
    "    def __init__(self, n_estimators=100):\n",
    "        \"\"\"\n",
    "        Initialize the Random Forest predictor for crop yield/production\n",
    "        \"\"\"\n",
    "        self.model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            random_state=global_seed,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "    def aggregate_grids(self, hrrr_batch):\n",
    "        \"\"\"\n",
    "        Aggregate features across grids using statistical measures\n",
    "        hrrr_batch shape: [batch, months, days, grids, parameters]\n",
    "        \"\"\"\n",
    "        # Move grids dimension to end for easier aggregation\n",
    "        # New shape: [batch, months, days, parameters, grids]\n",
    "        batch_data = hrrr_batch.permute(0, 1, 2, 4, 3)\n",
    "        \n",
    "        # Calculate statistics across grids\n",
    "        grid_mean = batch_data.mean(dim=-1)\n",
    "        grid_std = batch_data.std(dim=-1)\n",
    "        grid_min = batch_data.min(dim=-1)[0]\n",
    "        grid_max = batch_data.max(dim=-1)[0]\n",
    "        \n",
    "        # Concatenate all statistics\n",
    "        # Shape: [batch, months, days, parameters * 4]\n",
    "        aggregated = np.concatenate([\n",
    "            grid_mean.numpy(),\n",
    "            grid_std.numpy(),\n",
    "            grid_min.numpy(),\n",
    "            grid_max.numpy()\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Flatten all dimensions except batch\n",
    "        # Shape: [batch, months * days * parameters * 4]\n",
    "        return aggregated.reshape(aggregated.shape[0], -1)\n",
    "        \n",
    "    def train(self, hrrr_loader, usda_loader):\n",
    "        \"\"\"\n",
    "        Train the random forest model\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for (hrrr_batch, hrrr_fips, _), (usda_batch, usda_fips, _) in zip(hrrr_loader, usda_loader):\n",
    "            # Verify data alignment\n",
    "            assert all(h == u for h, u in zip(hrrr_fips, usda_fips)), \"HRRR and USDA FIPS mismatch\"\n",
    "            \n",
    "            # Aggregate grid features\n",
    "            X.append(self.aggregate_grids(hrrr_batch))\n",
    "            y.append(usda_batch.numpy())\n",
    "        \n",
    "        X = np.concatenate(X)\n",
    "        y = np.concatenate(y)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = X\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(X_scaled, y)\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        y_pred = self.model.predict(X_scaled)\n",
    "        \n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y, y_pred)),\n",
    "            'mae': mean_absolute_error(y, y_pred)\n",
    "        }\n",
    "    \n",
    "    def predict(self, hrrr_loader):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model\n",
    "        \"\"\"\n",
    "        X, fips = [], []\n",
    "        \n",
    "        for hrrr_batch, batch_fips, _ in hrrr_loader:\n",
    "            X.append(self.aggregate_grids(hrrr_batch))\n",
    "            fips.extend(batch_fips)\n",
    "        \n",
    "        X = np.concatenate(X)\n",
    "        X_scaled = X\n",
    "        \n",
    "        return self.model.predict(X_scaled), fips\n",
    "    \n",
    "    def evaluate(self, hrrr_loader, usda_loader):\n",
    "        \"\"\"\n",
    "        Evaluate the model performance\n",
    "        \"\"\"\n",
    "        all_predictions, pred_fips = self.predict(hrrr_loader)\n",
    "        \n",
    "        all_ground_truth, true_fips = [], []\n",
    "        for usda_batch, batch_fips, _ in usda_loader:\n",
    "            all_ground_truth.append(usda_batch.numpy())\n",
    "            true_fips.extend(batch_fips)\n",
    "            \n",
    "        all_ground_truth = np.concatenate(all_ground_truth)\n",
    "\n",
    "        # Verify data alignment\n",
    "        assert all(p == t for p, t in zip(pred_fips, true_fips)), \"Prediction and true FIPS mismatch\"\n",
    "        results = {}\n",
    "        for i, metric_name in enumerate([\"Production\", \"Yield\"]):\n",
    "            y_true = torch.from_numpy(all_ground_truth[:, i])\n",
    "            y_pred = torch.from_numpy(all_predictions[:, i])\n",
    "            mae = torch.abs(y_true - y_pred).mean()\n",
    "            mse = ((y_pred - y_true) ** 2).mean()\n",
    "            rmse = torch.sqrt(mse)\n",
    "            mape = (torch.abs(y_true - y_pred) / torch.abs(y_true)).mean() * 100\n",
    "            smape = 100 * (torch.abs(y_true - y_pred) / ((torch.abs(y_true) + torch.abs(y_pred)) / 2)).mean()\n",
    "            max_error = torch.abs(y_true - y_pred).max()\n",
    "            corr = torch.corrcoef(torch.stack((y_pred, y_true)))\n",
    "            metrics = {\n",
    "                'MAE': round(mae.item(), 2),\n",
    "                'MSE': round(mse.item(), 2),\n",
    "                'RMSE': round(rmse.item(), 2),\n",
    "                'MAPE': round(mape.item(), 2),\n",
    "                'SMAPE': round(smape.item(), 2)\n",
    "            }\n",
    "            results[metric_name] = metrics\n",
    "        return results\n",
    "    \n",
    "    def get_feature_importance(self, parameters, months):\n",
    "        \"\"\"\n",
    "        Get feature importance scores for different statistics and parameters\n",
    "        \"\"\"\n",
    "        importances = self.model.feature_importances_\n",
    "        n_days = 28  # As defined in HRRRComputedDataset\n",
    "        n_params = len(parameters)\n",
    "        n_stats = 4  # mean, std, min, max\n",
    "        \n",
    "        # Reshape importances to [months, days, parameters, stats]\n",
    "        shaped_imp = importances.reshape(len(months), n_days, n_params, n_stats)\n",
    "        \n",
    "        # Calculate importance per parameter and statistic\n",
    "        param_importance = shaped_imp.mean(axis=(0, 1))  # Average across months and days\n",
    "        \n",
    "        importance_dict = {\n",
    "            'parameters': {},\n",
    "            'statistics': {\n",
    "                'mean': shaped_imp[..., 0].mean(),\n",
    "                'std': shaped_imp[..., 1].mean(),\n",
    "                'min': shaped_imp[..., 2].mean(),\n",
    "                'max': shaped_imp[..., 3].mean()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for i, param in enumerate(parameters):\n",
    "            importance_dict['parameters'][param] = {\n",
    "                'mean': param_importance[i, 0],\n",
    "                'std': param_importance[i, 1],\n",
    "                'min': param_importance[i, 2],\n",
    "                'max': param_importance[i, 3]\n",
    "            }\n",
    "            \n",
    "        return importance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f2af12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:00:41.779240Z",
     "iopub.status.busy": "2024-10-25T06:00:41.778806Z",
     "iopub.status.idle": "2024-10-25T06:00:41.803005Z",
     "shell.execute_reply": "2024-10-25T06:00:41.801873Z"
    },
    "papermill": {
     "duration": 0.035002,
     "end_time": "2024-10-25T06:00:41.805656",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.770654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeCropPredictor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Decision Tree predictor for crop yield/production\n",
    "        \"\"\"\n",
    "        self.model = DecisionTreeRegressor(\n",
    "            random_state=global_seed\n",
    "        )\n",
    "        \n",
    "    def aggregate_grids(self, hrrr_batch):\n",
    "        \"\"\"\n",
    "        Aggregate features across grids using statistical measures\n",
    "        hrrr_batch shape: [batch, months, days, grids, parameters]\n",
    "        \"\"\"\n",
    "        # Move grids dimension to end for easier aggregation\n",
    "        # New shape: [batch, months, days, parameters, grids]\n",
    "        batch_data = hrrr_batch.permute(0, 1, 2, 4, 3)\n",
    "        \n",
    "        # Calculate statistics across grids\n",
    "        grid_mean = batch_data.mean(dim=-1)\n",
    "        grid_std = batch_data.std(dim=-1)\n",
    "        grid_min = batch_data.min(dim=-1)[0]\n",
    "        grid_max = batch_data.max(dim=-1)[0]\n",
    "        \n",
    "        # Concatenate all statistics\n",
    "        # Shape: [batch, months, days, parameters * 4]\n",
    "        aggregated = np.concatenate([\n",
    "            grid_mean.numpy(),\n",
    "            grid_std.numpy(),\n",
    "            grid_min.numpy(),\n",
    "            grid_max.numpy()\n",
    "        ], axis=-1)\n",
    "        \n",
    "        # Flatten all dimensions except batch\n",
    "        # Shape: [batch, months * days * parameters * 4]\n",
    "        return aggregated.reshape(aggregated.shape[0], -1)\n",
    "        \n",
    "    def train(self, hrrr_loader, usda_loader):\n",
    "        \"\"\"\n",
    "        Train the decision tree model\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for (hrrr_batch, hrrr_fips, _), (usda_batch, usda_fips, _) in zip(hrrr_loader, usda_loader):\n",
    "            # Verify data alignment\n",
    "            assert all(h == u for h, u in zip(hrrr_fips, usda_fips)), \"HRRR and USDA FIPS mismatch\"\n",
    "            \n",
    "            # Aggregate grid features\n",
    "            X.append(self.aggregate_grids(hrrr_batch))\n",
    "            y.append(usda_batch.numpy())\n",
    "        \n",
    "        X = np.concatenate(X)\n",
    "        y = np.concatenate(y)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = X\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(X_scaled, y)\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        y_pred = self.model.predict(X_scaled)\n",
    "        \n",
    "        return {\n",
    "            'rmse': np.sqrt(mean_squared_error(y, y_pred)),\n",
    "            'mae': mean_absolute_error(y, y_pred)\n",
    "        }\n",
    "    \n",
    "    def predict(self, hrrr_loader):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model\n",
    "        \"\"\"\n",
    "        X, fips = [], []\n",
    "        \n",
    "        for hrrr_batch, batch_fips, _ in hrrr_loader:\n",
    "            X.append(self.aggregate_grids(hrrr_batch))\n",
    "            fips.extend(batch_fips)\n",
    "        \n",
    "        X = np.concatenate(X)\n",
    "        X_scaled = X\n",
    "        \n",
    "        return self.model.predict(X_scaled), fips\n",
    "    \n",
    "    def evaluate(self, hrrr_loader, usda_loader):\n",
    "        \"\"\"\n",
    "        Evaluate the model performance\n",
    "        \"\"\"\n",
    "        all_predictions, pred_fips = self.predict(hrrr_loader)\n",
    "        \n",
    "        all_ground_truth, true_fips = [], []\n",
    "        for usda_batch, batch_fips, _ in usda_loader:\n",
    "            all_ground_truth.append(usda_batch.numpy())\n",
    "            true_fips.extend(batch_fips)\n",
    "            \n",
    "        all_ground_truth = np.concatenate(all_ground_truth)\n",
    "\n",
    "        # Verify data alignment\n",
    "        assert all(p == t for p, t in zip(pred_fips, true_fips)), \"Prediction and true FIPS mismatch\"\n",
    "        results = {}\n",
    "        for i, metric_name in enumerate([\"Production\", \"Yield\"]):\n",
    "            y_true = torch.from_numpy(all_ground_truth[:, i])\n",
    "            y_pred = torch.from_numpy(all_predictions[:, i])\n",
    "            mae = torch.abs(y_true - y_pred).mean()\n",
    "            mse = ((y_pred - y_true) ** 2).mean()\n",
    "            rmse = torch.sqrt(mse)\n",
    "            mape = (torch.abs(y_true - y_pred) / torch.abs(y_true)).mean() * 100\n",
    "            smape = 100 * (torch.abs(y_true - y_pred) / ((torch.abs(y_true) + torch.abs(y_pred)) / 2)).mean()\n",
    "            max_error = torch.abs(y_true - y_pred).max()\n",
    "            corr = torch.corrcoef(torch.stack((y_pred, y_true)))\n",
    "            metrics = {\n",
    "                'MAE': round(mae.item(), 2),\n",
    "                'MSE': round(mse.item(), 2),\n",
    "                'RMSE': round(rmse.item(), 2),\n",
    "                'MAPE': round(mape.item(), 2),\n",
    "                'SMAPE': round(smape.item(), 2)\n",
    "            }\n",
    "            results[metric_name] = metrics\n",
    "        return results\n",
    "    \n",
    "    def get_feature_importance(self, parameters, months):\n",
    "        \"\"\"\n",
    "        Get feature importance scores for different statistics and parameters\n",
    "        \"\"\"\n",
    "        importances = self.model.feature_importances_\n",
    "        n_days = 28  # As defined in HRRRComputedDataset\n",
    "        n_params = len(parameters)\n",
    "        n_stats = 4  # mean, std, min, max\n",
    "        \n",
    "        # Reshape importances to [months, days, parameters, stats]\n",
    "        shaped_imp = importances.reshape(len(months), n_days, n_params, n_stats)\n",
    "        \n",
    "        # Calculate importance per parameter and statistic\n",
    "        param_importance = shaped_imp.mean(axis=(0, 1))  # Average across months and days\n",
    "        \n",
    "        importance_dict = {\n",
    "            'parameters': {},\n",
    "            'statistics': {\n",
    "                'mean': shaped_imp[..., 0].mean(),\n",
    "                'std': shaped_imp[..., 1].mean(),\n",
    "                'min': shaped_imp[..., 2].mean(),\n",
    "                'max': shaped_imp[..., 3].mean()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for i, param in enumerate(parameters):\n",
    "            importance_dict['parameters'][param] = {\n",
    "                'mean': param_importance[i, 0],\n",
    "                'std': param_importance[i, 1],\n",
    "                'min': param_importance[i, 2],\n",
    "                'max': param_importance[i, 3]\n",
    "            }\n",
    "            \n",
    "        return importance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a85583",
   "metadata": {
    "papermill": {
     "duration": 0.005931,
     "end_time": "2024-10-25T06:00:41.818326",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.812395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34b4c9",
   "metadata": {
    "papermill": {
     "duration": 0.006016,
     "end_time": "2024-10-25T06:00:41.831876",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.825860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Winter Wheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db87ce42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:00:41.846427Z",
     "iopub.status.busy": "2024-10-25T06:00:41.845983Z",
     "iopub.status.idle": "2024-10-25T06:05:34.105942Z",
     "shell.execute_reply": "2024-10-25T06:05:34.104408Z"
    },
    "papermill": {
     "duration": 292.276921,
     "end_time": "2024-10-25T06:05:34.114924",
     "exception": false,
     "start_time": "2024-10-25T06:00:41.838003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WinterWheat\n",
      "{'Production': {'MAE': 1.37,\n",
      "                'MAPE': 9.93,\n",
      "                'MSE': 3.69,\n",
      "                'RMSE': 1.92,\n",
      "                'SMAPE': 10.91},\n",
      " 'Yield': {'MAE': 0.2, 'MAPE': 4.56, 'MSE': 0.06, 'RMSE': 0.25, 'SMAPE': 4.72}}\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/kaggle/input/cropnetv2\"\n",
    "\n",
    "train_config = \"train_config.json\"\n",
    "test_config = \"test_config.json\"\n",
    "train_hrrr_dataset = HRRRComputedDataset(base_dir,train_config)\n",
    "train_usda_dataset = USDACropDataset(base_dir,train_config,crop_type)\n",
    "\n",
    "test_hrrr_dataset = HRRRComputedDataset(base_dir,test_config)\n",
    "test_usda_dataset = USDACropDataset(base_dir,test_config,crop_type)\n",
    "\n",
    "# Create data loaders\n",
    "train_hrrr_loader = DataLoader(train_hrrr_dataset, batch_size=1, shuffle=False)\n",
    "train_usda_loader = DataLoader(train_usda_dataset, batch_size=1, shuffle=False)\n",
    "test_hrrr_loader = DataLoader(test_hrrr_dataset, batch_size=1, shuffle=False)\n",
    "test_usda_loader = DataLoader(test_usda_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "predictor = DecisionTreeCropPredictor()\n",
    "\n",
    "# Train and get metrics\n",
    "train_metrics = predictor.train(train_hrrr_loader, train_usda_loader)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = predictor.evaluate(test_hrrr_loader, test_usda_loader)\n",
    "print(crop_type)\n",
    "pprint(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68037ce7",
   "metadata": {
    "papermill": {
     "duration": 0.006095,
     "end_time": "2024-10-25T06:05:34.127167",
     "exception": false,
     "start_time": "2024-10-25T06:05:34.121072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cotton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f19088c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:05:34.141401Z",
     "iopub.status.busy": "2024-10-25T06:05:34.140980Z",
     "iopub.status.idle": "2024-10-25T06:05:34.159386Z",
     "shell.execute_reply": "2024-10-25T06:05:34.158348Z"
    },
    "papermill": {
     "duration": 0.028898,
     "end_time": "2024-10-25T06:05:34.162260",
     "exception": false,
     "start_time": "2024-10-25T06:05:34.133362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_config(years: List[int], state: str, state_ansi: str, fips: str, crop_type: str, grow_season: List[int]):\n",
    "\n",
    "    config = {\n",
    "        \"FIPS\": fips,\n",
    "        \"years\": years,\n",
    "        \"state\": state.upper(),\n",
    "        \"crop_type\": crop_type,\n",
    "        \"data\": {\n",
    "            \"HRRR\": {\n",
    "                \"short_term\": []\n",
    "            },\n",
    "            \"USDA\": [],\n",
    "            \"sentinel\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for year in years:\n",
    "        # HRRR data\n",
    "        hrrr_files = [\n",
    "            f\"HRRR/{year}/{state.upper()}/HRRR_{state_ansi}_{state.upper()}_{year}-{month:02d}.csv\"\n",
    "            for month in range(grow_season[0], grow_season[1] + 1)\n",
    "        ]\n",
    "        config[\"data\"][\"HRRR\"][\"short_term\"].append(hrrr_files)\n",
    "        \n",
    "        # USDA data\n",
    "        if crop_type==\"Soybeans\":\n",
    "            config[\"data\"][\"USDA\"].append(f\"USDA/{crop_type}/{year}/USDA_Soybean_County_{year}.csv\")\n",
    "        else:\n",
    "            config[\"data\"][\"USDA\"].append(f\"USDA/{crop_type}/{year}/USDA_{crop_type}_County_{year}.csv\")\n",
    "        \n",
    "        # Sentinel data\n",
    "        quarters = [\n",
    "            (f\"{year}-01-01\", f\"{year}-03-31\"),\n",
    "            (f\"{year}-04-01\", f\"{year}-06-30\"),\n",
    "            (f\"{year}-07-01\", f\"{year}-09-30\"),\n",
    "            (f\"{year}-10-01\", f\"{year}-12-31\")\n",
    "        ]\n",
    "        \n",
    "        sentinel_files = []\n",
    "        for start, end in quarters:\n",
    "            quarter_start = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "            quarter_end = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "            if (grow_season[0] <= quarter_start.month <= grow_season[1]) or \\\n",
    "               (grow_season[0] <= quarter_end.month <= grow_season[1]):\n",
    "                sentinel_files.append(f\"AG/{state.upper()}/{year}/Agriculture_{state_ansi}_{state.upper()}_{start}_{end}.h5\")\n",
    "        \n",
    "        config[\"data\"][\"sentinel\"].append(sentinel_files)\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Train\n",
    "years = list(range(2018,2022))\n",
    "state = \"AL\"\n",
    "state_ansi = \"01\"\n",
    "fips = ['01003', '01015', '01019', '01031', '01039', '01045', '01047', '01053', '01061', \n",
    "        '01067', '01069', '01077', '01079', '01083', '01089', '01097', '01099', '01117'] \n",
    "\n",
    "crop_type = \"Cotton\"\n",
    "grow_season = [4, 9]  # April to September\n",
    "\n",
    "train_config = make_config(years, state, state_ansi, fips, crop_type, grow_season)\n",
    "with open('train_config.json', 'w') as file:\n",
    "    json.dump(train_config, file)\n",
    "\n",
    "# Test\n",
    "years = [2022]\n",
    "test_config = make_config(years,  state, state_ansi, fips, crop_type, grow_season)\n",
    "with open('test_config.json', 'w') as file:\n",
    "    json.dump(test_config, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25bb8c17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:05:34.177759Z",
     "iopub.status.busy": "2024-10-25T06:05:34.177336Z",
     "iopub.status.idle": "2024-10-25T06:07:47.539307Z",
     "shell.execute_reply": "2024-10-25T06:07:47.538010Z"
    },
    "papermill": {
     "duration": 133.378581,
     "end_time": "2024-10-25T06:07:47.547321",
     "exception": false,
     "start_time": "2024-10-25T06:05:34.168740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cotton\n",
      "{'Production': {'MAE': 0.44,\n",
      "                'MAPE': 4.46,\n",
      "                'MSE': 0.32,\n",
      "                'RMSE': 0.56,\n",
      "                'SMAPE': 4.33},\n",
      " 'Yield': {'MAE': 0.19, 'MAPE': 2.76, 'MSE': 0.05, 'RMSE': 0.21, 'SMAPE': 2.76}}\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/kaggle/input/cropnetv2\"\n",
    "\n",
    "train_config = \"train_config.json\"\n",
    "test_config = \"test_config.json\"\n",
    "train_hrrr_dataset = HRRRComputedDataset(base_dir,train_config)\n",
    "train_usda_dataset = USDACropDataset(base_dir,train_config,crop_type)\n",
    "\n",
    "test_hrrr_dataset = HRRRComputedDataset(base_dir,test_config)\n",
    "test_usda_dataset = USDACropDataset(base_dir,test_config,crop_type)\n",
    "\n",
    "# Create data loaders\n",
    "train_hrrr_loader = DataLoader(train_hrrr_dataset, batch_size=1, shuffle=False)\n",
    "train_usda_loader = DataLoader(train_usda_dataset, batch_size=1, shuffle=False)\n",
    "test_hrrr_loader = DataLoader(test_hrrr_dataset, batch_size=1, shuffle=False)\n",
    "test_usda_loader = DataLoader(test_usda_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "predictor = DecisionTreeCropPredictor()\n",
    "\n",
    "# Train and get metrics\n",
    "train_metrics = predictor.train(train_hrrr_loader, train_usda_loader)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = predictor.evaluate(test_hrrr_loader, test_usda_loader)\n",
    "print(crop_type)\n",
    "pprint(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e603e",
   "metadata": {
    "papermill": {
     "duration": 0.005945,
     "end_time": "2024-10-25T06:07:47.559524",
     "exception": false,
     "start_time": "2024-10-25T06:07:47.553579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Corn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc0f3333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:07:47.574812Z",
     "iopub.status.busy": "2024-10-25T06:07:47.574333Z",
     "iopub.status.idle": "2024-10-25T06:07:47.593088Z",
     "shell.execute_reply": "2024-10-25T06:07:47.591185Z"
    },
    "papermill": {
     "duration": 0.02997,
     "end_time": "2024-10-25T06:07:47.595890",
     "exception": false,
     "start_time": "2024-10-25T06:07:47.565920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_config(years: List[int], state: str, state_ansi: str, fips: str, crop_type: str, grow_season: List[int]):\n",
    "\n",
    "    config = {\n",
    "        \"FIPS\": fips,\n",
    "        \"years\": years,\n",
    "        \"state\": state.upper(),\n",
    "        \"crop_type\": crop_type,\n",
    "        \"data\": {\n",
    "            \"HRRR\": {\n",
    "                \"short_term\": []\n",
    "            },\n",
    "            \"USDA\": [],\n",
    "            \"sentinel\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for year in years:\n",
    "        # HRRR data\n",
    "        hrrr_files = [\n",
    "            f\"HRRR/{year}/{state.upper()}/HRRR_{state_ansi}_{state.upper()}_{year}-{month:02d}.csv\"\n",
    "            for month in range(grow_season[0], grow_season[1] + 1)\n",
    "        ]\n",
    "        config[\"data\"][\"HRRR\"][\"short_term\"].append(hrrr_files)\n",
    "        \n",
    "        # USDA data\n",
    "        if crop_type==\"Soybeans\":\n",
    "            config[\"data\"][\"USDA\"].append(f\"USDA/{crop_type}/{year}/USDA_Soybean_County_{year}.csv\")\n",
    "        else:\n",
    "            config[\"data\"][\"USDA\"].append(f\"USDA/{crop_type}/{year}/USDA_{crop_type}_County_{year}.csv\")\n",
    "        \n",
    "        # Sentinel data\n",
    "        quarters = [\n",
    "            (f\"{year}-01-01\", f\"{year}-03-31\"),\n",
    "            (f\"{year}-04-01\", f\"{year}-06-30\"),\n",
    "            (f\"{year}-07-01\", f\"{year}-09-30\"),\n",
    "            (f\"{year}-10-01\", f\"{year}-12-31\")\n",
    "        ]\n",
    "        \n",
    "        sentinel_files = []\n",
    "        for start, end in quarters:\n",
    "            quarter_start = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "            quarter_end = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "            if (grow_season[0] <= quarter_start.month <= grow_season[1]) or \\\n",
    "               (grow_season[0] <= quarter_end.month <= grow_season[1]):\n",
    "                sentinel_files.append(f\"AG/{state.upper()}/{year}/Agriculture_{state_ansi}_{state.upper()}_{start}_{end}.h5\")\n",
    "        \n",
    "        config[\"data\"][\"sentinel\"].append(sentinel_files)\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Train\n",
    "years = list(range(2018,2022))\n",
    "state = \"IL\"\n",
    "state_ansi = \"17\"\n",
    "fips = ['17007', '17011', '17015', '17017', '17019', '17021', '17025', '17027',  \n",
    "        '17037', '17049', '17053', '17055', '17057', '17059', '17061', '17063', '17073', \n",
    "        '17075', '17077', '17081', '17085', '17089', '17093', '17095', '17101', '17103', \n",
    "        '17105', '17107', '17113', '17115', '17117', '17119', '17121', '17123', '17133', \n",
    "        '17135', '17139', '17141', '17143', '17147', '17157', '17163', '17167', '17169', \n",
    "        '17173', '17175', '17177', '17179', '17189', '17193', '17195', '17201', '17203'] \n",
    "crop_type = \"Corn\"\n",
    "grow_season = [4, 9]  # April to September\n",
    "\n",
    "train_config = make_config(years, state, state_ansi, fips, crop_type, grow_season)\n",
    "with open('train_config.json', 'w') as file:\n",
    "    json.dump(train_config, file)\n",
    "\n",
    "# Test\n",
    "years = [2022]\n",
    "test_config = make_config(years,  state, state_ansi, fips, crop_type, grow_season)\n",
    "with open('test_config.json', 'w') as file:\n",
    "    json.dump(test_config, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "164987ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:07:47.610660Z",
     "iopub.status.busy": "2024-10-25T06:07:47.610217Z",
     "iopub.status.idle": "2024-10-25T06:13:35.417479Z",
     "shell.execute_reply": "2024-10-25T06:13:35.416333Z"
    },
    "papermill": {
     "duration": 347.82331,
     "end_time": "2024-10-25T06:13:35.425587",
     "exception": false,
     "start_time": "2024-10-25T06:07:47.602277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corn\n",
      "{'Production': {'MAE': 0.63,\n",
      "                'MAPE': 3.73,\n",
      "                'MSE': 0.61,\n",
      "                'RMSE': 0.78,\n",
      "                'SMAPE': 3.71},\n",
      " 'Yield': {'MAE': 0.08, 'MAPE': 1.54, 'MSE': 0.01, 'RMSE': 0.1, 'SMAPE': 1.54}}\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/kaggle/input/cropnetv2\"\n",
    "\n",
    "train_config = \"train_config.json\"\n",
    "test_config = \"test_config.json\"\n",
    "train_hrrr_dataset = HRRRComputedDataset(base_dir,train_config)\n",
    "train_usda_dataset = USDACropDataset(base_dir,train_config,crop_type)\n",
    "\n",
    "test_hrrr_dataset = HRRRComputedDataset(base_dir,test_config)\n",
    "test_usda_dataset = USDACropDataset(base_dir,test_config,crop_type)\n",
    "\n",
    "# Create data loaders\n",
    "train_hrrr_loader = DataLoader(train_hrrr_dataset, batch_size=1, shuffle=False)\n",
    "train_usda_loader = DataLoader(train_usda_dataset, batch_size=1, shuffle=False)\n",
    "test_hrrr_loader = DataLoader(test_hrrr_dataset, batch_size=1, shuffle=False)\n",
    "test_usda_loader = DataLoader(test_usda_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "predictor = DecisionTreeCropPredictor()\n",
    "\n",
    "# Train and get metrics\n",
    "train_metrics = predictor.train(train_hrrr_loader, train_usda_loader)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = predictor.evaluate(test_hrrr_loader, test_usda_loader)\n",
    "print(crop_type)\n",
    "pprint(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d233f79",
   "metadata": {
    "papermill": {
     "duration": 0.006627,
     "end_time": "2024-10-25T06:13:35.439400",
     "exception": false,
     "start_time": "2024-10-25T06:13:35.432773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Soybeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a96adc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:13:35.454838Z",
     "iopub.status.busy": "2024-10-25T06:13:35.454351Z",
     "iopub.status.idle": "2024-10-25T06:13:35.474244Z",
     "shell.execute_reply": "2024-10-25T06:13:35.472910Z"
    },
    "papermill": {
     "duration": 0.031127,
     "end_time": "2024-10-25T06:13:35.476972",
     "exception": false,
     "start_time": "2024-10-25T06:13:35.445845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_config(years: List[int], state: str, state_ansi: str, fips: str, crop_type: str, grow_season: List[int]):\n",
    "\n",
    "    config = {\n",
    "        \"FIPS\": fips,\n",
    "        \"years\": years,\n",
    "        \"state\": state.upper(),\n",
    "        \"crop_type\": crop_type,\n",
    "        \"data\": {\n",
    "            \"HRRR\": {\n",
    "                \"short_term\": []\n",
    "            },\n",
    "            \"USDA\": [],\n",
    "            \"sentinel\": []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for year in years:\n",
    "        # HRRR data\n",
    "        hrrr_files = [\n",
    "            f\"HRRR/{year}/{state.upper()}/HRRR_{state_ansi}_{state.upper()}_{year}-{month:02d}.csv\"\n",
    "            for month in range(grow_season[0], grow_season[1] + 1)\n",
    "        ]\n",
    "        config[\"data\"][\"HRRR\"][\"short_term\"].append(hrrr_files)\n",
    "        \n",
    "        # USDA data\n",
    "        if crop_type==\"Soybeans\":\n",
    "            config[\"data\"][\"USDA\"].append(f\"USDA/{crop_type}/{year}/USDA_Soybean_County_{year}.csv\")\n",
    "        else:\n",
    "            config[\"data\"][\"USDA\"].append(f\"USDA/{crop_type}/{year}/USDA_{crop_type}_County_{year}.csv\")\n",
    "        \n",
    "        # Sentinel data\n",
    "        quarters = [\n",
    "            (f\"{year}-01-01\", f\"{year}-03-31\"),\n",
    "            (f\"{year}-04-01\", f\"{year}-06-30\"),\n",
    "            (f\"{year}-07-01\", f\"{year}-09-30\"),\n",
    "            (f\"{year}-10-01\", f\"{year}-12-31\")\n",
    "        ]\n",
    "        \n",
    "        sentinel_files = []\n",
    "        for start, end in quarters:\n",
    "            quarter_start = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "            quarter_end = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "            if (grow_season[0] <= quarter_start.month <= grow_season[1]) or \\\n",
    "               (grow_season[0] <= quarter_end.month <= grow_season[1]):\n",
    "                sentinel_files.append(f\"AG/{state.upper()}/{year}/Agriculture_{state_ansi}_{state.upper()}_{start}_{end}.h5\")\n",
    "        \n",
    "        config[\"data\"][\"sentinel\"].append(sentinel_files)\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Train\n",
    "years = list(range(2018,2022))\n",
    "state = \"IL\"\n",
    "state_ansi = \"17\"\n",
    "fips = ['17005', '17007', '17009', '17011', '17015', '17019', '17025', '17027', '17037', \n",
    "        '17045', '17049', '17053', '17055', '17057', '17059', '17063', '17073', '17075', '17077', \n",
    "        '17081', '17089', '17091', '17095', '17101', '17103', '17105', '17113', '17115', '17117', \n",
    "        '17119', '17121', '17129', '17133', '17139', '17141', '17143', '17145', '17153', '17157', \n",
    "        '17163', '17167', '17173', '17177', '17179', '17189', '17193', '17197', '17201', '17203']\n",
    "\n",
    "crop_type = \"Soybeans\"\n",
    "grow_season = [4, 9]  # April to September\n",
    "\n",
    "train_config = make_config(years, state, state_ansi, fips, crop_type, grow_season)\n",
    "with open('train_config.json', 'w') as file:\n",
    "    json.dump(train_config, file)\n",
    "\n",
    "# Test\n",
    "years = [2022]\n",
    "test_config = make_config(years,  state, state_ansi, fips, crop_type, grow_season)\n",
    "with open('test_config.json', 'w') as file:\n",
    "    json.dump(test_config, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81419807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-25T06:13:35.492068Z",
     "iopub.status.busy": "2024-10-25T06:13:35.491654Z",
     "iopub.status.idle": "2024-10-25T06:19:01.429560Z",
     "shell.execute_reply": "2024-10-25T06:19:01.428034Z"
    },
    "papermill": {
     "duration": 325.954224,
     "end_time": "2024-10-25T06:19:01.438018",
     "exception": false,
     "start_time": "2024-10-25T06:13:35.483794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soybeans\n",
      "{'Production': {'MAE': 0.7,\n",
      "                'MAPE': 4.46,\n",
      "                'MSE': 0.71,\n",
      "                'RMSE': 0.84,\n",
      "                'SMAPE': 4.52},\n",
      " 'Yield': {'MAE': 0.13, 'MAPE': 3.15, 'MSE': 0.03, 'RMSE': 0.16, 'SMAPE': 3.18}}\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/kaggle/input/cropnetv2\"\n",
    "\n",
    "train_config = \"train_config.json\"\n",
    "test_config = \"test_config.json\"\n",
    "train_hrrr_dataset = HRRRComputedDataset(base_dir,train_config)\n",
    "train_usda_dataset = USDACropDataset(base_dir,train_config,crop_type)\n",
    "\n",
    "test_hrrr_dataset = HRRRComputedDataset(base_dir,test_config)\n",
    "test_usda_dataset = USDACropDataset(base_dir,test_config,crop_type)\n",
    "\n",
    "# Create data loaders\n",
    "train_hrrr_loader = DataLoader(train_hrrr_dataset, batch_size=1, shuffle=False)\n",
    "train_usda_loader = DataLoader(train_usda_dataset, batch_size=1, shuffle=False)\n",
    "test_hrrr_loader = DataLoader(test_hrrr_dataset, batch_size=1, shuffle=False)\n",
    "test_usda_loader = DataLoader(test_usda_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "predictor = DecisionTreeCropPredictor()\n",
    "\n",
    "# Train and get metrics\n",
    "train_metrics = predictor.train(train_hrrr_loader, train_usda_loader)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = predictor.evaluate(test_hrrr_loader, test_usda_loader)\n",
    "print(crop_type)\n",
    "pprint(test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5852641,
     "sourceId": 9594770,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1129.736118,
   "end_time": "2024-10-25T06:19:02.971722",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-25T06:00:13.235604",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
