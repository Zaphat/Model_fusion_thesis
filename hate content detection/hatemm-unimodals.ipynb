{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9979035,"sourceType":"datasetVersion","datasetId":6140254}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:39:37.635741Z","iopub.execute_input":"2024-11-27T14:39:37.636029Z","iopub.status.idle":"2024-11-27T14:39:47.769258Z","shell.execute_reply.started":"2024-11-27T14:39:37.636002Z","shell.execute_reply":"2024-11-27T14:39:47.768099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model, ViTModel, ViTImageProcessor\nfrom sentence_transformers import SentenceTransformer\nimport torchaudio\nimport h5py\nimport torch.nn.functional as F\nimport math\n# Set environment variable\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n\nglobal_seed = 0\nrandom.seed(global_seed)\nnp.random.seed(global_seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:39:47.771537Z","iopub.execute_input":"2024-11-27T14:39:47.771926Z","iopub.status.idle":"2024-11-27T14:40:07.238172Z","shell.execute_reply.started":"2024-11-27T14:39:47.771884Z","shell.execute_reply":"2024-11-27T14:40:07.237452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AudioHateClassifier(nn.Module):\n    def __init__(self, hidden_size=768, num_heads=2, num_layers=2, dropout=0.1, train_option=\"finetune\", seed=0):\n        assert train_option in [\"finetune\",\"transfer\"]\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # Initialize audio extractor\n        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n        \n        if train_option == \"transfer\":\n            for param in self.wav2vec_model.parameters():\n                param.requires_grad = False\n\n        # Audio projection and position embedding\n        self.audio_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.audio_pos_embed = nn.Parameter(torch.randn(1, 1, hidden_size))\n        \n        # Attention layers\n        self.attention_layers = nn.ModuleList([\n            nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n            for _ in range(num_layers)\n        ])\n        \n        self.layer_norms = nn.ModuleList([\n            nn.LayerNorm(hidden_size)\n            for _ in range(num_layers)\n        ])\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        # Classification head\n        self.output_head = nn.Sequential(\n            nn.Linear(hidden_size, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def _extract_audio_features(self, audio_path):\n        # Same as original\n        waveform, sample_rate = torchaudio.load(audio_path)\n        if waveform.shape[0] > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)\n        \n        if sample_rate != self.wav2vec_processor.feature_extractor.sampling_rate:\n            resampler = torchaudio.transforms.Resample(sample_rate, \n                self.wav2vec_processor.feature_extractor.sampling_rate)\n            waveform = resampler(waveform)\n\n        if torch.abs(waveform).float().mean().item() * 100 > 90:\n            return None\n            \n        input_values = self.wav2vec_processor(waveform.squeeze().numpy(), \n            sampling_rate=self.wav2vec_processor.feature_extractor.sampling_rate, \n            return_tensors=\"pt\").input_values.to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.wav2vec_model(input_values)\n            features = outputs.last_hidden_state.squeeze(0)\n            \n        return features\n\n    def forward(self, audio_path):\n        features = self._extract_audio_features(audio_path)\n        if features is None:\n            return torch.tensor([0.5]).to(self.device).detach().clone().requires_grad_(True)\n            \n        features = self.audio_proj(features) + self.audio_pos_embed\n        \n        # Apply attention layers\n        for attention, norm in zip(self.attention_layers, self.layer_norms):\n            attended_features, _ = attention(features, features, features)\n            features = features + self.dropout(attended_features)\n            features = norm(features)\n        \n        # Pool and classify\n        pooled_features = features.squeeze().mean(dim=0)\n        output = self.output_head(pooled_features)\n        return output\n\n    def predict(self, audio_path):\n        self.eval()\n        with torch.no_grad():\n            output = self.forward(audio_path)\n            return (output > 0.5).item(), output.item()\n\nclass ImageHateClassifier(nn.Module):\n    def __init__(self, hidden_size=768, num_heads=2, num_layers=2, dropout=0.1, train_option=\"finetune\", seed=0):\n        assert train_option in [\"finetune\",\"transfer\"]\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # Initialize image extractor\n        self.vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n        self.vit_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n        \n        if train_option == \"transfer\":\n            for param in self.vit_model.parameters():\n                param.requires_grad = False\n\n        # Image projection and position embedding\n        self.image_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.image_pos_embed = nn.Parameter(torch.randn(1, 1, hidden_size))\n        \n        # Attention layers\n        self.attention_layers = nn.ModuleList([\n            nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n            for _ in range(num_layers)\n        ])\n        \n        self.layer_norms = nn.ModuleList([\n            nn.LayerNorm(hidden_size)\n            for _ in range(num_layers)\n        ])\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        # Classification head\n        self.output_head = nn.Sequential(\n            nn.Linear(hidden_size, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def _extract_image_features(self, h5_path, batch_size=32):\n        # Same as original\n        with h5py.File(h5_path, 'r') as f:\n            frames = f['frames'][:]\n        if len(frames.shape) == 3:\n            frames = np.stack([frames] * 3, axis=-1)\n        \n        all_features = []\n        for i in range(0, len(frames), batch_size):\n            batch_frames = frames[i:i + batch_size]\n            inputs = self.vit_processor(images=batch_frames, return_tensors=\"pt\").pixel_values.to(self.device)\n            \n            with torch.no_grad():\n                outputs = self.vit_model(inputs)\n                features = outputs.pooler_output\n                all_features.append(features)\n        \n        return torch.cat(all_features, dim=0) if all_features else None\n\n    def forward(self, image_path):\n        features = self._extract_image_features(image_path)\n        if features is None:\n            with torch.no_grad():\n                return torch.tensor([0.5]).to(self.device).detach().clone().requires_grad_(True)\n            \n        features = self.image_proj(features) + self.image_pos_embed\n        \n        # Apply attention layers\n        for attention, norm in zip(self.attention_layers, self.layer_norms):\n            attended_features, _ = attention(features, features, features)\n            features = features + self.dropout(attended_features)\n            features = norm(features)\n        \n        # Pool and classify\n        pooled_features = features.squeeze(0).mean(dim=0)\n        output = self.output_head(pooled_features)\n        return output\n\n    def predict(self, image_path):\n        self.eval()\n        with torch.no_grad():\n            output = self.forward(image_path)\n            return (output > 0.5).item(), output.item()\n\nclass TextHateClassifier(nn.Module):\n    def __init__(self, hidden_size=768, num_heads=2, num_layers=2, dropout=0.1, train_option=\"finetune\", seed=0):\n        assert train_option in [\"finetune\",\"transfer\"]\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # Initialize text extractor\n        self.text_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n        \n        if train_option == \"transfer\":\n            for param in self.text_model.parameters():\n                param.requires_grad = False\n\n        # Text projection and position embedding\n        self.text_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n        self.text_pos_embed = nn.Parameter(torch.randn(1, 1, hidden_size))\n        \n        # Attention layers\n        self.attention_layers = nn.ModuleList([\n            nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n            for _ in range(num_layers)\n        ])\n        \n        self.layer_norms = nn.ModuleList([\n            nn.LayerNorm(hidden_size)\n            for _ in range(num_layers)\n        ])\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        # Classification head\n        self.output_head = nn.Sequential(\n            nn.Linear(hidden_size, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def _extract_text_features(self, txt_path):\n        # Same as original\n        with open(txt_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n        \n        sentences = [s.strip() for s in text.split('. ') if s.strip()]\n        if not sentences:\n            return None\n            \n        with torch.no_grad():\n            embeddings = self.text_model.encode(sentences, convert_to_tensor=True).to(self.device)\n        \n        return embeddings\n\n    def forward(self, text_path):\n        features = self._extract_text_features(text_path)\n        if features is None:\n            return torch.tensor([0.5]).to(self.device).detach().clone().requires_grad_(True)\n            \n        features = self.text_proj(features) + self.text_pos_embed\n        \n        # Apply attention layers\n        for attention, norm in zip(self.attention_layers, self.layer_norms):\n            attended_features, _ = attention(features, features, features)\n            features = features + self.dropout(attended_features)\n            features = norm(features)\n        \n        # Pool and classify\n        pooled_features = features.squeeze(0).mean(dim=0)\n        output = self.output_head(pooled_features)\n        return output\n\n    def predict(self, text_path):\n        self.eval()\n        with torch.no_grad():\n            output = self.forward(text_path)\n            return (output > 0.5).item(), output.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:40:07.239308Z","iopub.execute_input":"2024-11-27T14:40:07.239863Z","iopub.status.idle":"2024-11-27T14:40:07.269480Z","shell.execute_reply.started":"2024-11-27T14:40:07.239825Z","shell.execute_reply":"2024-11-27T14:40:07.268646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nfrom tqdm import tqdm\nimport os\nimport gc\nimport time\n\nclass HateVideoDataset(Dataset):\n    def __init__(self, dataframe, base_path):\n        self.data = dataframe\n        self.base_path = base_path\n        \n    def __len__(self):\n        return len(self.data)        \n        \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        audio_path = os.path.join(self.base_path, 'audio', row['audio'])\n        image_path = os.path.join(self.base_path, 'image_sequences', row['imageseq'])\n        text_path = os.path.join(self.base_path, 'transcripts', row['transcript'])\n        label = torch.tensor([row['label']], dtype=torch.float)\n        return {\n            'audio_path': audio_path,\n            'image_path': image_path,\n            'text_path': text_path,\n            'label': label\n        }\n\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=1e-5):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0\n\ndef validate_models(models, val_loader, criterion):\n    losses = {name: 0.0 for name in models.keys()}\n    \n    for model_name, model in models.items():\n        model.eval()\n        with torch.no_grad():\n            for data in tqdm(val_loader):\n                torch.cuda.empty_cache()\n                gc.collect()\n                label = data['label'].to(\"cuda\")\n                path_key = f\"{model_name}_path\"\n                outputs = model(data[path_key])\n                losses[model_name] += criterion[model_name](outputs, label).item()\n                \n        losses[model_name] /= len(val_loader)\n    \n    return losses\n\ndef train_multi_modal(models, train_loader, val_loader, criterion, optimizers,\n                     schedulers=None, num_epochs=20, patience=5, \n                     checkpoint_dir=\"checkpoints\"):\n    torch.set_grad_enabled(True)\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    early_stoppers = {name: EarlyStopping(patience=patience) for name in models.keys()}\n    best_losses = {name: float('inf') for name in models.keys()}\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Training phase\n        for model_name, model in models.items():\n            model.train()\n            running_loss = 0.0\n            print(model_name)\n            for data in tqdm(train_loader):\n                label = data['label'].to(\"cuda\")\n                path_key = f\"{model_name}_path\"\n                torch.cuda.empty_cache()\n                gc.collect()\n                optimizers[model_name].zero_grad()\n                outputs = model(data[path_key])\n                loss = criterion[model_name](outputs.float(), label.float())\n                loss.backward()\n                optimizers[model_name].step()\n                running_loss += loss.item()\n            \n            epoch_loss = running_loss / len(train_loader)\n            print(f\"{model_name} Training Loss: {epoch_loss:.4f}\")\n        \n        # Validation phase\n        val_losses = validate_models(models, val_loader, criterion)\n        \n        # Update schedulers and check early stopping\n        should_stop = []\n        for model_name in models.keys():\n            if schedulers and model_name in schedulers:\n                schedulers[model_name].step(val_losses[model_name])\n            \n            early_stoppers[model_name](val_losses[model_name])\n            print(f\"{model_name} Validation Loss: {val_losses[model_name]:.4f}\")\n            \n            if val_losses[model_name] < best_losses[model_name]:\n                best_losses[model_name] = val_losses[model_name]\n                torch.save(models[model_name].state_dict(), \n                         os.path.join(checkpoint_dir, f\"{model_name}_best.pth\"))\n                print(f\"{model_name} model saved\")\n            \n            if early_stoppers[model_name].early_stop:\n                should_stop.append(model_name)\n        \n        if should_stop:\n            print(f\"Early stopping for models: {', '.join(should_stop)}\")\n            if len(should_stop) == len(models):\n                break\n    \n    return models\n\ndef evaluate_multi_modal(models, test_loader):\n    results = {}\n    \n    for model_name, model in models.items():\n        model.eval()\n        y_true = []\n        y_pred = []\n        \n        with torch.no_grad():\n            for data in tqdm(test_loader):\n                torch.cuda.empty_cache()\n                gc.collect()\n                label = data['label'].to(\"cuda\")\n                path_key = f\"{model_name}_path\"\n                outputs = model(data[path_key])\n                y_true.extend(label.cpu().numpy())\n                y_pred.extend(outputs.cpu().numpy())\n        \n        y_true = np.array(y_true)\n        y_pred = (np.array(y_pred) > 0.5).astype(int)\n        accuracy = accuracy_score(y_true, y_pred)\n        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n        roc_auc = roc_auc_score(y_true, y_pred)\n        results[model_name] = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'roc_auc': roc_auc\n            }\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:40:07.272418Z","iopub.execute_input":"2024-11-27T14:40:07.272829Z","iopub.status.idle":"2024-11-27T14:40:07.311204Z","shell.execute_reply.started":"2024-11-27T14:40:07.272764Z","shell.execute_reply":"2024-11-27T14:40:07.310483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = \"/kaggle/input/mmhate/HateMM\"\ntorch.cuda.empty_cache()\ngc.collect()\n\ndef get_duration(file):\n    import wave\n    with wave.open(file, 'r') as f:\n        frames = f.getnframes()\n        rate = f.getframerate()\n        duration = frames / float(rate)\n        return int(duration)+ (duration>int(duration))\n\nfiles = []  \nfor file in os.listdir('/kaggle/input/mmhate/HateMM/audio/'):\n    if file.endswith('.wav'):\n        if get_duration('/kaggle/input/mmhate/HateMM/audio/' + file) <= 180: # 3 minutes\n            files.append(file)\n\ndf = pd.DataFrame(columns=['audio', 'imageseq', 'transcript', 'label'])\n\nfor file in files:\n  filename = file.split('.')[0]\n  df = pd.concat([df, pd.DataFrame({'audio': [filename + '.wav'], 'imageseq': [filename + '.h5'], 'transcript': [filename + '.txt'], 'label': [0 if \"non\" in filename else 1]})], ignore_index=True)\n                                                                                                                                           \n\ntrain, test = train_test_split(df, test_size=0.15, random_state=1, stratify=df['label'])\ntrain, val = train_test_split(train, test_size=0.176, random_state=0, stratify=train['label'])\n\n# Create datasets\ntrain_dataset = HateVideoDataset(train, base_path)\nval_dataset = HateVideoDataset(val, base_path)\ntest_dataset = HateVideoDataset(test, base_path)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=None, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=None)\ntest_loader = DataLoader(test_dataset, batch_size=None)\n\n# Initialize models\nmodels = {\n    'audio': AudioHateClassifier().to(\"cuda\"),\n    'image': ImageHateClassifier().to(\"cuda\"),\n    'text': TextHateClassifier().to(\"cuda\")\n}\n\n# Initialize optimizers and criteria\nnum_epochs = 20\nlearning_rate = 1e-4\noptimizers = {\n    name: torch.optim.AdamW(model.parameters(),lr=learning_rate) \n    for name, model in models.items()\n}\nschedulers = {\n    name: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer) \n    for name, optimizer in optimizers.items()\n}\ncriteria = {name: nn.BCELoss() for name, _ in models.items()}\n\n# Train models\ntrained_models = train_multi_modal(\n    models, train_loader, val_loader, criteria, \n    optimizers, schedulers\n)\n\n# Evaluate\nresults = evaluate_multi_modal(trained_models, test_loader)\nfor k,v in results.items(): \n    print(k)\n    print(v)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:40:07.312383Z","iopub.execute_input":"2024-11-27T14:40:07.312711Z"}},"outputs":[],"execution_count":null}]}