{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9979035,"sourceType":"datasetVersion","datasetId":6140254}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U -q sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model, ViTModel, ViTImageProcessor\nfrom sentence_transformers import SentenceTransformer\nimport torchaudio\nimport h5py\nimport torch.nn.functional as F\nfrom torch.nn.functional import pad\nimport math\n# Set environment variable\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n\nglobal_seed = 0\nrandom.seed(global_seed)\nnp.random.seed(global_seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HateVideoClassifier(nn.Module):\n    def __init__(self, hidden_size=768, num_heads=2, num_layers=2, dropout=0.1, train_option=\"finetune\", seed=0):\n        \n        assert train_option in [\"finetune\",\"transfer\"], ValueError(\"Not a correct training option\")\n        # Ensure reproducible\n\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # Initialize feature extractors\n        self._init_extractors(train_option==\"transfer\")\n        \n        # Modality-specific projections to ensure consistent dimensionality\n        self.audio_proj = nn.Linear(hidden_size, hidden_size,bias=False)\n        self.image_proj = nn.Linear(hidden_size, hidden_size,bias=False)\n        self.text_proj = nn.Linear(hidden_size, hidden_size,bias=False)\n        \n        # Modality-specific positional embeddings\n        self.audio_pos_embed = nn.Parameter(torch.randn(1, 1, hidden_size))\n        self.image_pos_embed = nn.Parameter(torch.randn(1, 1, hidden_size))\n        self.text_pos_embed = nn.Parameter(torch.randn(1, 1, hidden_size))\n              \n        # Missing modality tokens (learnable)\n        self.missing_audio_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n        self.missing_image_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n        self.missing_text_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n        \n        # Cross-modal attention layers\n        self.cross_attention = nn.ModuleList([\n            nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout, batch_first=True)\n            for _ in range(num_layers)\n        ])\n        \n        # Layer normalization and dropout\n        self.layer_norms = nn.ModuleList([\n            nn.LayerNorm(hidden_size)\n            for _ in range(num_layers)\n        ])\n        \n        # Modality-specific layer norms\n        self.audio_norm = nn.LayerNorm(hidden_size)\n        self.image_norm = nn.LayerNorm(hidden_size)\n        self.text_norm = nn.LayerNorm(hidden_size)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        # Final classification layers\n        self.output_head = nn.Sequential(\n            nn.Linear(hidden_size, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n    def _init_extractors(self,transfer=False): # Initialize the feature extractors for each modality\n        # Audio\n        self.wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n        self.wav2vec_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n        \n        # Image\n        self.vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n        self.vit_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n        \n        # Text\n        self.text_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n        if transfer:\n            for param in self.wav2vec_model.parameters():\n                param.requires_grad = False\n            for param in self.vit_model.parameters():\n                param.requires_grad = False\n            for param in self.text_model.parameters():\n                param.requires_grad = False\n\n    def _extract_audio_features(self, audio_path): # Extract audio features using Wav2Vec2\n        \n        # Load and preprocess audio\n        waveform, sample_rate = torchaudio.load(audio_path)\n        \n        # Convert to mono if stereo\n        if waveform.shape[0] > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)\n        \n        # Resample if necessary\n        if sample_rate != self.wav2vec_processor.feature_extractor.sampling_rate:\n            resampler = torchaudio.transforms.Resample(\n                sample_rate, \n                self.wav2vec_processor.feature_extractor.sampling_rate\n            )\n            waveform = resampler(waveform)\n\n        # Check for silence\n        threshold = 0.01\n        silent_frames = torch.abs(waveform) < threshold\n        silent_percentage = silent_frames.float().mean().item() * 100\n        \n        if silent_percentage > 90:\n            return None\n            \n        # Process audio\n        input_values = self.wav2vec_processor(\n            waveform.squeeze().numpy(), \n            sampling_rate=self.wav2vec_processor.feature_extractor.sampling_rate, \n            return_tensors=\"pt\"\n        ).input_values.to(self.device)\n        \n        # Extract features\n        with torch.no_grad():\n            outputs = self.wav2vec_model(input_values)\n            features = outputs.last_hidden_state.squeeze(0)\n            \n        return features\n\n    def _extract_image_features(self, h5_path, batch_size=32): # Extract image features using ViT\n        with h5py.File(h5_path, 'r') as f:\n            frames = f['frames'][:]\n\n        # Convert frames to RGB if necessary\n        if len(frames.shape) == 3:\n            frames = np.stack([frames] * 3, axis=-1)\n        \n        # Process frames in batches\n        all_features = []\n        for i in range(0, len(frames), batch_size):\n            batch_frames = frames[i:i + batch_size]\n            \n            inputs = self.vit_processor(\n                images=batch_frames,\n                return_tensors=\"pt\"\n            ).pixel_values.to(self.device)\n            \n            with torch.no_grad():\n                outputs = self.vit_model(inputs)\n                features = outputs.pooler_output\n                all_features.append(features)\n        \n        return torch.cat(all_features, dim=0) if all_features else None\n\n    def _extract_text_features(self, txt_path): # Extract text features using SentenceTransformer\n        with open(txt_path, 'r', encoding='utf-8') as f:\n            text = f.read()\n        \n        sentences = [s.strip() for s in text.split('. ') if s.strip()]\n        \n        if not sentences:\n            return None\n            \n        with torch.no_grad():\n            embeddings = self.text_model.encode(sentences, convert_to_tensor=True).to(self.device)\n        \n        return embeddings\n\n    def _apply_modality_projection(self, features, proj_layer, pos_embed):\n        if features is None:\n            return None\n            \n        features = proj_layer(features)\n        return features + pos_embed\n        \n    def _handle_missing_modality(self, features, missing_token, norm_layer):\n        if features is None:\n            return missing_token\n        return norm_layer(features)\n\n    def _fuse_modalities(self, audio_features, image_features, text_features):\n        # Get batch size from non-None features\n        batch_size = None\n        for features in [audio_features, image_features, text_features]:\n            if features is not None:\n                batch_size = features.size(0)\n                break\n        \n        if batch_size is None:\n            batch_size = 1\n            \n        # Get sequence lengths\n        audio_len = 1 if audio_features is None else audio_features.size(1)\n        image_len = 1 if image_features is None else image_features.size(1)\n        text_len = 1 if text_features is None else text_features.size(1)\n\n        # Create attention mask based on presence/absence of modalities\n        attention_mask = torch.zeros(\n            batch_size, audio_len + image_len + text_len,\n            device=self.device,\n            dtype=torch.bool\n        )\n        \n        # Set mask values based on modality presence\n        current_pos = 0\n        \n        # Audio mask\n        if audio_features is not None:\n            attention_mask[:, current_pos:current_pos + audio_len] = True\n        current_pos += audio_len\n        \n        # Image mask\n        if image_features is not None:\n            attention_mask[:, current_pos:current_pos + image_len] = True\n        current_pos += image_len\n        \n        # Text mask\n        if text_features is not None:\n            attention_mask[:, current_pos:current_pos + text_len] = True\n\n        # Handle missing modalities with correct batch dimension\n        audio_features = self._handle_missing_modality(audio_features, \n                                                     self.missing_audio_token.repeat(batch_size, 1, 1), \n                                                     self.audio_norm)\n        image_features = self._handle_missing_modality(image_features, \n                                                     self.missing_image_token.repeat(batch_size, 1, 1), \n                                                     self.image_norm)\n        text_features = self._handle_missing_modality(text_features, \n                                                    self.missing_text_token.repeat(batch_size, 1, 1), \n                                                    self.text_norm)\n        \n        # Concatenate all modalities\n        combined_features = torch.cat([\n            audio_features,\n            image_features,\n            text_features\n        ], dim=1)\n        \n        # Apply cross-modal attention layers\n        features = combined_features\n        for attention, norm in zip(self.cross_attention, self.layer_norms):\n            torch.cuda.empty_cache()\n            gc.collect()\n            # Apply attention with correct mask\n            attended_features, _ = attention(\n                query=features,\n                key=features,\n                value=features,\n                key_padding_mask=~attention_mask,  # PyTorch attention expects False for valid positions\n                need_weights=False\n            )\n            \n            # Apply residual connection and dropout\n            features = features + self.dropout(attended_features)\n            features = norm(features)\n        \n        # Pool features using the mask\n        mask_expanded = attention_mask.unsqueeze(-1).float()\n        masked_sum = (features * mask_expanded).sum(dim=1)\n        mask_sum = mask_expanded.sum(dim=1).clamp(min=1.0)  # Prevent division by zero\n        pooled_features = masked_sum / mask_sum\n        \n        return pooled_features\n\n    def forward(self, audio_path, image_path, text_path):\n        # Extract features for each modality\n        audio_features = self._extract_audio_features(audio_path)\n        image_features = self._extract_image_features(image_path)\n        text_features = self._extract_text_features(text_path)\n        \n        # Apply projections and embeddings\n        audio_features = self._apply_modality_projection(\n            audio_features, self.audio_proj, self.audio_pos_embed)\n        image_features = self._apply_modality_projection(\n            image_features, self.image_proj, self.image_pos_embed)\n        text_features = self._apply_modality_projection(\n            text_features, self.text_proj, self.text_pos_embed)\n        \n        # Fuse modalities using cross-attention\n        fused_features = self._fuse_modalities(audio_features, image_features, text_features)\n        # Final classification\n        output = self.output_head(fused_features).squeeze(0)\n        del audio_features, image_features, text_features, fused_features\n        torch.cuda.empty_cache()\n        return output\n\n    def predict(self, audio_path, image_path, text_path):\n        self.eval()\n        with torch.no_grad():\n            output = self.forward(audio_path, image_path, text_path)\n            return (output > 0.5).item(), output.item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\nfrom tqdm import tqdm\nimport os\nimport gc\nimport time\n\nclass HateVideoDataset(Dataset):\n    def __init__(self, dataframe, base_path):\n        self.data = dataframe\n        self.base_path = base_path\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # Construct full paths\n        audio_path = os.path.join(self.base_path, 'audio', row['audio'])\n        image_path = os.path.join(self.base_path, 'image_sequences', row['imageseq'])\n        text_path = os.path.join(self.base_path, 'transcripts', row['transcript'])\n        label = torch.tensor([row['label']], dtype=torch.float)\n        return {\n            'audio_path': audio_path,\n            'image_path': image_path,\n            'text_path': text_path,\n            'label': label\n        }\n\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.counter = 0\n\ndef validate(model, val_loader, criterion):\n  torch.cuda.empty_cache()\n  gc.collect()\n  model.eval()\n  running_loss = 0.0\n\n  with torch.no_grad():\n    for data in tqdm(val_loader):\n        torch.cuda.empty_cache()\n        gc.collect()\n        audio_path = data['audio_path']\n        image_path = data['image_path']\n        text_path = data['text_path']\n        label = data['label'].to(\"cuda\")\n        outputs = model(audio_path, image_path, text_path)\n        loss = criterion(outputs, label)\n        \n        running_loss += loss.item()\n\n  val_loss = running_loss / len(val_loader)\n  return val_loss\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, \n                scheduler=None, num_epochs=50, \n              patience=3, checkpoint_path=\"best_model.pth\"):\n\n    model.train()\n\n    early_stopping = EarlyStopping(patience=patience)\n\n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print('-' * 10)\n        running_loss = 0.0\n        for data in tqdm(train_loader):\n            torch.cuda.empty_cache()\n            gc.collect()\n            audio_path = data['audio_path']\n            image_path = data['image_path']\n            text_path = data['text_path']\n            label = data['label'].to(\"cuda\")\n            optimizer.zero_grad()\n            outputs = model(audio_path, image_path, text_path)\n            loss = criterion(outputs, label)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        epoch_loss = running_loss / len(train_loader)\n        print(f\"Training Loss: {epoch_loss}\")\n\n        val_loss = validate(model, val_loader, criterion)\n        print(f\"Validation Loss: {val_loss}\")\n\n        if scheduler is not None:\n            scheduler.step(val_loss)\n\n        early_stopping(val_loss)\n\n        if early_stopping.early_stop:\n            print(\"Early stopping\")\n            break\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), checkpoint_path)\n            print(\"Model saved\")\n\n    return model\n\ndef evaluate_model(model, test_loader):\n    \n    model.eval()\n    y_true = []\n    y_pred = []\n    \n    with torch.no_grad():\n        for data in tqdm(test_loader):\n            torch.cuda.empty_cache()\n            gc.collect()\n            audio_path = data['audio_path']\n            image_path = data['image_path']\n            text_path = data['text_path']\n            label = data['label'].to(\"cuda\")\n            outputs = model(audio_path, image_path, text_path)\n            y_true.extend(label.cpu().numpy())\n            y_pred.extend(outputs.cpu().numpy())\n            \n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    y_pred = (y_pred > 0.5).astype(int)\n    \n    accuracy = accuracy_score(y_true, y_pred)\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n    roc_auc = roc_auc_score(y_true, y_pred)\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'roc_auc': roc_auc\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = \"/kaggle/input/mmhate/HateMM\"\n\ndef get_duration(file):\n    import wave\n    with wave.open(file, 'r') as f:\n        frames = f.getnframes()\n        rate = f.getframerate()\n        duration = frames / float(rate)\n        return int(duration)+ (duration>int(duration))\n\nfiles = []  \n\nfor file in os.listdir('/kaggle/input/mmhate/HateMM/audio/'):\n    if file.endswith('.wav'):\n        if get_duration('/kaggle/input/mmhate/HateMM/audio/' + file) <= 180: # 3 minutes\n            files.append(file)\n\ndf = pd.DataFrame(columns=['audio', 'imageseq', 'transcript', 'label'])\n\nfor file in files:\n  filename = file.split('.')[0]\n  df = pd.concat([df, pd.DataFrame({'audio': [filename + '.wav'], 'imageseq': [filename + '.h5'], 'transcript': [filename + '.txt'], 'label': [0 if \"non\" in filename else 1]})], ignore_index=True)\n                                                                                                                                           \n\ntrain, test = train_test_split(df, test_size=0.15, random_state=0, stratify=df['label'])\ntrain, val = train_test_split(train, test_size=0.176, random_state=0, stratify=train['label'])\n\nnum_epochs = 20\nlearning_rate = 1e-4\n\n\n# Create datasets\ntrain_dataset = HateVideoDataset(train, base_path)\nval_dataset = HateVideoDataset(val, base_path)\ntest_dataset = HateVideoDataset(test, base_path)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=None, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=None)\ntest_loader = DataLoader(test_dataset, batch_size=None)\n\n# Initialize model and training components\nmodel = HateVideoClassifier().to(\"cuda\")\n\ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.1, patience=3, verbose=True\n)\n\n# Train model\nmodel = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs)\n\n# Evaluate model\nresults = evaluate_model(model, test_loader)\nprint(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}